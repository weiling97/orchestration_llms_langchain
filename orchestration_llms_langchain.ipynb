{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7573abd8-6674-4952-8641-8ce2a978387e",
   "metadata": {},
   "source": [
    "## Install packages and import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0996bc96-204e-4c32-9963-edb0a821dd63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install Vertex AI SDK & Other dependencies\n",
    "\n",
    "!sudo apt -y -qq install tesseract-ocr\n",
    "!sudo apt -y -qq install libtesseract-dev\n",
    "!sudo apt-get -y -qq install poppler-utils #required by PyPDF2 for page count and other pdf utilities\n",
    "!sudo apt-get -y -qq install python-dev libxml2-dev libxslt1-dev antiword unrtf poppler-utils tesseract-ocr flac ffmpeg lame libmad0 libsox-fmt-mp3 sox libjpeg-dev swig\n",
    "!pip install langchain langchain-community langchain-core langchain-google-vertexai pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b36a8e4-ea7b-410b-ae11-1275b662ce1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Automatically restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1e22b08-4adc-4fb9-95b5-ec7fb7f6a261",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import urllib\n",
    "import warnings\n",
    "from pathlib import Path as p\n",
    "\n",
    "import pandas as pd\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_google_vertexai import ChatVertexAI\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c874f7c1-0757-4080-ab92-2869630b42f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Load the model and prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d03b7b86-4b03-43e7-80f3-a2465a78da56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load the pre-trained text generation model named \"gemini-1.5-pro\" using \"ChatVertexAI\" class.\n",
    "llm = ChatVertexAI(\n",
    "    model=\"gemini-2.5-pro\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    max_retries=6,\n",
    "    stop=None,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee11c7a0-960f-4a91-be60-e8883ec898ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF downloaded to: /home/jupyter/data/practitioners_guaide_to_mlops_whitepaper.pdf\n"
     ]
    }
   ],
   "source": [
    "#Download a PDF file from specified URL and save it in \"data\" directory.\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "\n",
    "url = 'https://services.google.com/fh/files/misc/practitioners_guide_to_mlops_whitepaper.pdf'\n",
    "\n",
    "data_dir = Path('data')\n",
    "data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pdf_path = data_dir/\"practitioners_guaide_to_mlops_whitepaper.pdf\"\n",
    "\n",
    "urllib.request.urlretrieve(url, pdf_path)\n",
    "print(f\"PDF downloaded to: {pdf_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "765ff8de-16c5-4c82-801a-7745c9262cf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load the PDF file and split it into individual pages.\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(str(pdf_path))\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "099a8a38-d07e-46df-b499-5f71befe0334",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign 16.1 (Macintosh)', 'creationdate': '2021-05-11T22:05:38+03:00', 'moddate': '2021-05-11T22:05:50+03:00', 'trapped': '/False', 'source': 'data/practitioners_guaide_to_mlops_whitepaper.pdf', 'total_pages': 37, 'page': 2, 'page_label': '3'}\n",
      "\n",
      "Executive summary\n",
      "Across industries, DevOps and DataOps have been widely adopted as methodologies to improve quality and re-\n",
      "duce the time to market of software engineering and data engineering initiatives. With the rapid growth in machine \n",
      "learning (ML) systems, similar approaches need to be developed in the context of ML engineering, which handle the \n",
      "unique complexities of the practical applications of ML. This is the domain of MLOps. MLOps is a set of standard-\n",
      "ized processes and technology capabilities for building, deploying, and operationalizing ML systems rapidly and \n",
      "reliably.]\n",
      "We previously published Google Cloud’s AI Adoption Framework to provide guidance for technology leaders who \n",
      "want to build an effective artificial intelligence (AI) capability in order to transform their business. That framework \n",
      "covers AI challenges around people, data, technology, and process, structured in six different themes: learn, lead, \n",
      "access, secure, scale, and automate. \n",
      "The current document takes a deeper dive into the themes of scale and automate to illustrate the requirements for \n",
      "building and operationalizing ML systems. Scale concerns the extent to which you use cloud managed ML services \n",
      "that scale with large amounts of data and large numbers of data processing and ML jobs, with reduced operational \n",
      "overhead. Automate concerns the extent to which you are able to deploy, execute, and operate technology for data \n",
      "processing and ML pipelines in production efficiently, frequently, and reliably.\n",
      "We outline an MLOps framework that defines core processes and technical capabilities. Organizations can use this \n",
      "framework to help establish mature MLOps practices for building and operationalizing ML systems. Adopting the \n",
      "framework can help organizations improve collaboration between teams, improve the reliability and scalability of ML \n",
      "systems, and shorten development cycle times. These benefits in turn drive innovation and help gain overall busi-\n",
      "ness value from investments in ML.\n",
      "This document is intended for technology leaders and enterprise architects who want to understand MLOps. It’s also \n",
      "for teams who want details about what MLOps looks like in practice. The document assumes that readers are famil-\n",
      "iar with basic machine learning concepts and with development and deployment practices such as CI/CD.\n",
      "The document is in two parts. The first part, an overview of the MLOps lifecycle, is for all readers. It introduces \n",
      "MLOps processes and capabilities and why they’re important for successful adoption of ML-based systems.\n",
      "The second part is a deep dive on the MLOps processes and capabilities. This part is for readers who want to un-\n",
      "derstand the concrete details of tasks like running a continuous training pipeline, deploying a model, and monitoring \n",
      "predictive performance of an ML model.\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(f\"{pages[2].metadata}\\n\")\n",
    "print(pages[2].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3684e68c-4a1b-466e-8fee-b7660c5915c8",
   "metadata": {},
   "source": [
    "## 2. Generate summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de464893-f82f-4be1-a37e-d8e4d0f57d7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Prompt design with Stuffing Chain\n",
    "\n",
    "prompt_template = \"\"\"Write a concise summary of the following text delimited by triple backquotes.\n",
    "              Return your response in bullet points which covers the key points of the text.\n",
    "              ```{text}```\n",
    "              BULLET POINT SUMMARY:\n",
    "  \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24bec1e2-dbec-447f-a4f1-1565784623df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Of course. Here is a summary of the white paper \"Practitioners guide to MLOps: A framework for continuous delivery and automation of machine learning.\"\\n\\n### Executive Summary\\n\\nThis white paper presents a comprehensive framework for MLOps (Machine Learning Operations), a methodology designed to build, deploy, and operationalize Machine Learning (ML) systems efficiently and reliably. The authors argue that many ML projects fail to move from pilot to production because they lack standardized, automated processes. MLOps addresses this by applying principles from DevOps and DataOps to the unique challenges of the ML lifecycle, such as managing data, tracking experiments, and monitoring models for performance degradation.\\n\\nThe framework is intended to help organizations improve collaboration, increase the reliability and scalability of their ML systems, and shorten development cycles, ultimately driving more business value from their AI/ML investments.\\n\\n### The Core Problem MLOps Solves\\n\\nMany organizations struggle to deploy ML models due to:\\n*   **Manual and one-off work:** Lack of reusable and reproducible components.\\n*   **Integration issues:** Difficult handoffs between data scientists and IT/operations teams.\\n*   **Model degradation:** Models in production often break or become less accurate as the data they encounter in the real world (\"serving data\") drifts away from the data they were trained on.\\n*   **Lack of governance:** Difficulty in tracking, versioning, and managing a growing number of models.\\n\\nMLOps provides a structured approach to overcome these challenges by unifying ML system development with ML system operations.\\n\\n### The MLOps Lifecycle\\n\\nThe paper defines the MLOps lifecycle as seven integrated and iterative processes:\\n\\n1.  **ML Development:** The initial phase of experimentation where data scientists explore data, prototype models, and develop a reproducible training procedure (the training pipeline).\\n2.  **Training Operationalization:** The process of automating the packaging, testing, and deployment of the training pipeline itself using CI/CD (Continuous Integration/Continuous Delivery) practices.\\n3.  **Continuous Training (CT):** The automated execution of the training pipeline to retrain the model. This can be triggered on a schedule, by the availability of new data, or when model performance decay is detected.\\n4.  **Model Deployment:** The process of packaging, testing, and deploying a trained model to a production environment. This often involves progressive delivery techniques like canary or shadow deployments.\\n5.  **Prediction Serving:** The live model in production, serving predictions to consumers via online (real-time), batch (offline), or streaming methods.\\n6.  **Continuous Monitoring:** Actively tracking the deployed model\\'s performance. This includes monitoring both operational efficiency (latency, errors) and predictive effectiveness (detecting data drift, concept drift, and performance decay).\\n7.  **Data and Model Management:** A central, cross-cutting function for governing all ML artifacts (datasets, features, models, metadata) to ensure auditability, traceability, reusability, and compliance.\\n\\n### Core MLOps Capabilities\\n\\nTo support the lifecycle, organizations need a set of core technical capabilities, which can be provided by an integrated platform or a combination of tools. Key capabilities include:\\n\\n*   **Experimentation:** Tools for collaborative data analysis and model prototyping (e.g., notebooks integrated with Git).\\n*   **Data Processing:** Scalable tools to prepare and transform data for both training and serving.\\n*   **Model Training & Evaluation:** Services for efficient, scalable model training, hyperparameter tuning, and performance evaluation.\\n*   **Model Serving:** Infrastructure to serve models with low latency (online) or high throughput (batch).\\n*   **Online Experimentation:** The ability to run A/B tests or multi-armed bandit tests to compare model performance on live traffic.\\n*   **Model Monitoring:** Tools to detect data skew, concept drift, and other anomalies in production.\\n*   **ML Pipelines:** An orchestration service to automate and manage the steps in the ML workflow.\\n*   **Model Registry:** A central repository to version, track, and govern the lifecycle of ML models.\\n*   **Dataset and Feature Repository:** A centralized system to manage, share, and version data and features, which helps ensure consistency between training and serving and reduces redundant work.\\n*   **ML Metadata and Artifact Tracking:** A foundational capability that logs all information about ML runs—including data used, parameters, and results—to ensure reproducibility and lineage tracking.\\n\\n### Conclusion\\n\\nThe paper concludes that delivering business value with ML requires more than just building a good model. It requires an **integrated, end-to-end system** that can continuously adapt to a changing environment. By adopting the MLOps framework of processes and capabilities, organizations can streamline their workflow from development to production, leading to more reliable, scalable, and valuable ML systems.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set up a summarization chain using the stuff method. Incorporate the model loaded earlier into the summarization chain to enhance the quality of the summary.\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Summarize this content: {context}\")\n",
    "chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "result = chain.invoke({\"context\": pages})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de950df5-b9e9-48fa-900a-09bb46657928",
   "metadata": {},
   "source": [
    "## 3. Add tools to the LLM for multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ea98878d-d529-4e71-8867-7f1addd41b54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiply\n",
      "Multiply two numbers.\n",
      "{'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a tool called 'multiply' that takes two integers, 'a' and 'b', as inputs, performs the multiplication (a * b), and returns the result.\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "print(multiply.name)\n",
    "print(multiply.description)\n",
    "print(multiply.args)\n",
    "\n",
    "multiply.invoke({\"a\": 3, \"b\": 12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6071ad9f-f764-4eac-a5b2-4c7bcd583269",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Add the tools to the LLM created earlier and invoke it with the following query. Print the result in the console.\n",
    "from langchain_core.messages import HumanMessage, ToolMessage, AIMessage\n",
    "query = \"What is 3 * 12?\"\n",
    "messages = [HumanMessage(query)]\n",
    "\n",
    "llm_with_tools = llm.bind_tools([multiply])\n",
    "ai_msg = llm_with_tools.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b9e0caff-57f4-4782-be24-d2050aae602b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Iterate through the tools in the response, invoke the tools and append the response to the messages object.\n",
    "llm_with_tools.invoke(messages).tool_calls\n",
    "messages.append(ai_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "433d86ac-087c-4ba5-a4ba-2c38b3fb99d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is 3 * 12?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='', additional_kwargs={'function_call': {'name': 'multiply', 'arguments': '{\"a\": 3.0, \"b\": 12.0}'}}, response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 21, 'candidates_token_count': 5, 'total_token_count': 252, 'prompt_tokens_details': [{'modality': 1, 'token_count': 21}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 5}], 'thoughts_token_count': 226, 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -2.4901525497436525, 'model_name': 'gemini-2.5-pro'}, id='run--8a46a23b-1199-44be-9b1a-44e9b9be9de1-0', tool_calls=[{'name': 'multiply', 'args': {'a': 3.0, 'b': 12.0}, 'id': '129ff52d-8c73-4baf-930a-e19e95aae92c', 'type': 'tool_call'}], usage_metadata={'input_tokens': 21, 'output_tokens': 5, 'total_tokens': 252, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 226}}),\n",
       " ToolMessage(content='36', tool_call_id='129ff52d-8c73-4baf-930a-e19e95aae92c')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Invoke the LLM with the solution of the tool and the original message and print the final user response.\n",
    "for tool_call in ai_msg.tool_calls:\n",
    "    selected_tool = {\"multiply\": multiply}[tool_call[\"name\"].lower()]\n",
    "    tool_output = selected_tool.invoke(tool_call[\"args\"])\n",
    "    messages.append(ToolMessage(tool_output, tool_call_id=tool_call[\"id\"]))\n",
    "                    \n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803dde61-5587-4b49-a348-1aae34639184",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is 3 * 12?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='', additional_kwargs={'function_call': {'name': 'multiply', 'arguments': '{\"a\": 3.0, \"b\": 12.0}'}}, response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 21, 'candidates_token_count': 5, 'total_token_count': 252, 'prompt_tokens_details': [{'modality': 1, 'token_count': 21}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 5}], 'thoughts_token_count': 226, 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -2.4901525497436525, 'model_name': 'gemini-2.5-pro'}, id='run--8a46a23b-1199-44be-9b1a-44e9b9be9de1-0', tool_calls=[{'name': 'multiply', 'args': {'a': 3.0, 'b': 12.0}, 'id': '129ff52d-8c73-4baf-930a-e19e95aae92c', 'type': 'tool_call'}], usage_metadata={'input_tokens': 21, 'output_tokens': 5, 'total_tokens': 252, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 226}}),\n",
       " ToolMessage(content='36', tool_call_id='129ff52d-8c73-4baf-930a-e19e95aae92c'),\n",
       " HumanMessage(content='how about 13*14?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='', additional_kwargs={'function_call': {'name': 'multiply', 'arguments': '{\"b\": 14.0, \"a\": 13.0}'}}, response_metadata={'is_blocked': False, 'safety_ratings': [], 'usage_metadata': {'prompt_token_count': 39, 'candidates_token_count': 5, 'total_token_count': 169, 'prompt_tokens_details': [{'modality': 1, 'token_count': 39}], 'candidates_tokens_details': [{'modality': 1, 'token_count': 5}], 'thoughts_token_count': 125, 'cached_content_token_count': 0, 'cache_tokens_details': []}, 'finish_reason': 'STOP', 'avg_logprobs': -1.0481566429138183, 'model_name': 'gemini-2.5-pro'}, id='run--feec845c-47a7-4217-b15e-0a08d57749c0-0', tool_calls=[{'name': 'multiply', 'args': {'b': 14.0, 'a': 13.0}, 'id': 'd86598d2-e3dd-4c88-897a-2a33e8131fad', 'type': 'tool_call'}], usage_metadata={'input_tokens': 39, 'output_tokens': 5, 'total_tokens': 169, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 125}}),\n",
       " ToolMessage(content='182', tool_call_id='d86598d2-e3dd-4c88-897a-2a33e8131fad')]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Additional query\n",
    "query2 = \"how about 13*14?\"\n",
    "messages.append(HumanMessage(query2))\n",
    "\n",
    "ai_msg = llm_with_tools.invoke(messages)\n",
    "messages.append(ai_msg)\n",
    "\n",
    "for tool_call in ai_msg.tool_calls:\n",
    "    selected_tool = {\"multiply\": multiply}[tool_call[\"name\"].lower()]\n",
    "    tool_output = selected_tool.invoke(tool_call[\"args\"])\n",
    "    messages.append(ToolMessage(tool_output, tool_call_id=tool_call[\"id\"]))\n",
    "                    \n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb2b882-f6ac-4344-aaf1-c5be9fef3feb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
